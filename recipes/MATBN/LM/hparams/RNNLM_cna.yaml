output_folder: !ref results/RNNLM_cna
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
num_workers: 4

data_folder: results/prepare_cna

tokenizer_file: results/tokenizer_time_rnnlm/8000_char.model

tokenizer: !new:sentencepiece.SentencePieceProcessor

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
  collect_in: !ref <output_folder>/tokenizer
  loadables:
    tokenizer: !ref <tokenizer>
  paths:
    tokenizer: !ref <tokenizer_file>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>

# Training parameters
number_of_epochs: 50
batch_size: 128
lr: 0.001
accumulation_steps: 1
ckpt_interval_minutes: 15

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

# Dataloader options
train_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  shuffle: True

valid_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>

test_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>

# Model parameters
emb_size: 128
activation: !name:torch.nn.LeakyReLU
dropout: 0.2
rnn_layers: 2
rnn_neurons: 1024
dnn_blocks: 1
dnn_neurons: 256

# Outputs
output_neurons: 8000
blank_index: 0
bos_index: 1
eos_index: 2

model: !new:speechbrain.lobes.models.RNNLM.RNNLM
  output_neurons: !ref <output_neurons>
  embedding_dim: !ref <emb_size>
  activation: !ref <activation>
  dropout: !ref <dropout>
  rnn_layers: !ref <rnn_layers>
  rnn_neurons: !ref <rnn_neurons>
  dnn_blocks: !ref <dnn_blocks>
  dnn_neurons: !ref <dnn_neurons>

modules:
  model: !ref <model>

lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
  initial_value: !ref <lr>
  improvement_threshold: 0.0025
  annealing_factor: 0.8
  patient: 0

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    scheduler: !ref <lr_annealing>
    counter: !ref <epoch_counter>

log_softmax: !new:speechbrain.nnet.activations.Softmax
  apply_log: True

optimizer: !name:torch.optim.Adam
  lr: !ref <lr>
  betas: (0.9, 0.98)
  eps: 0.000000001

compute_cost: !name:speechbrain.nnet.losses.nll_loss
